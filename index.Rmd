---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Tia Frederick

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

  I used a dataset from Project 1, which is called data3. The dataset is a combination of the datasets "USArrests: Violent Crime Rates by US State" and "Anscombe: U.S. State Public-School Expenditures." US Arressts contains statistics in arrests per 100,000 residents for assault, murder, and rape in each of the 50 U.S. states in 1973, and the variables are labeled as 'Assault', 'Murder', and 'Rape', respectively. The dataset also provides the percent of the population living in urban areas, labeled as 'UrbanPop'. The Anscombe dataset contains statistics for the U.S. states plus Washington D.C. in 1970 for per-capita education expenditures in dollars ('education' variable), per-capita income ('income' variable), proportion under 18 years old per 1000 residents ('young' variable), and urban proportion per 1000 ('urban' variable). The USArrest dataset has 50 observations and 5 variables, and the Anscombe dataset has 51 observations and 5 variables, and the variable name for the states column is called "X1" for both datasets.
  The finalized dataset for data3 from Project 1 has 50 rows and 11 columns. The variables are: States, education, income, young, urban, Murder, Assault, Rape, UrbanPercent, Total_Arrests, and high_low_UrbanPercent. The variables, Total_Arrests and high_low_UrbanPercent, were mutated into data3 in Project1. "Total_Arrests" was generated by adding up the data in the crime columns. For the high_low_UrbanPercent variable, an urban percentage is categorized as high if it is greater than or equal to 60% and a low urban percentage is lower than 60%. One of my interesting findings from the previous project was that there were 33 states that are categorized has a high urban percentage and 17 states with a low urban percentage.



```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
# if your dataset needs tidying, do so here
# any other code here


#write.csv( file="data3.csv")
data3 <- read_csv("data3.csv")
data3
data3 <- data3 %>% select(-1)
data3


```

### Cluster Analysis

```{R}
library(cluster)

Data3 <- data3 %>% select(-1) 
Data3 <- Data3 %>% select(-10)
Data3

sil_width <- vector()
for(i in 2:10){
  kms <- kmeans(Data3, centers=i)
  sil <- silhouette(kms$cluster,dist(Data3))
  sil_width[i] <- mean(sil[,3])
}
ggplot() + geom_line(aes(x=1:10,y=sil_width)) + scale_x_continuous(name = "k", breaks=1:10)
data3_pam <- data3 %>% pam(k=2)
data3_pam$silinfo$avg.width
data3_pam


#Clustering with three measurements

clust_dat<-data3%>%dplyr::select(UrbanPercent,income, young)
set.seed(322)
pam1<-clust_dat%>%pam(k=2)
pam1

pamclust<-clust_dat%>%mutate(cluster=as.factor(pam1$clustering)) #save the cluster solution in your dataset
pamclust%>%ggplot(aes(UrbanPercent,income,high_low_UrbanPercent, color=cluster))+geom_point() #visualize it

pamclust%>%group_by(cluster)%>%summarize_if(is.numeric,mean,na.rm=T)

data3%>%slice(pam1$id.med)

#Clustering with all eight measurements

final <-data3 %>% select(-States) %>% select(-high_low_UrbanPercent) %>% scale %>% as.data.frame
pam2 <- final %>% pam(2)

#now that we ran PAM, save the cluster assignment in the dataset
final <- final %>% mutate(cluster=as.factor(pam2$clustering))

library(GGally)
ggpairs(final, aes(color=cluster))

```

Based on the first line graph, the best number of clusters with PAM based on silhouette width is 2. The overall average silhouette width after running PAM analysis is 0.51585 which shows that a decent structure has been found with ID 12 (Idaho) and ID 35 (Ohio) as the medoids. Idaho and Ohio are the most similar on Assault and Education and most different on income. At first I tested out clustering with only three measurements, but at the end I visualized all pairwise combinations of all eight measurements.
    
    
### Dimensionality Reduction with PCA

```{R}

data31<- data3 %>% select(-States) 
data31 <- data31 %>% select(-high_low_UrbanPercent)
data31
data31_nums <- data3 %>% select_if(is.numeric) %>% scale
rownames(data31_nums)<-data31$Name
data31_pca<-princomp(data31_nums)
names(data31_pca)

#sdev contains the square root of the eigenvalue/variance explained
#loadings contains the eigenvectors/principal components
#scores contains the new variables (scores on principal components)

#Summarize results. summary() (with loadings=T) on the pca object gives you everything (except new PC scores)
summary(data31_pca, loadings=T)

#To decide how many PCs to keep, plot proportion of variance explained by each PC (from largest eigenvalue to smallest): a "scree plot"


eigval<-data31_pca$sdev^2 #square to convert SDs to eigenvalues
varprop=round(eigval/sum(eigval), 2) #proportion of var explained by each PC
ggplot() + geom_bar(aes(y=varprop, x=1:9), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:9)) + 
  geom_text(aes(x=1:9, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10)


#How many PCs to keep? Rules of thumb
# 1. Pick PCs until scree plot flattens (sometimes hard to tell where "elbow" is)
# 2. Pick PCs until cumulative proportion of variance is > 80%
# 3. Pick PCs whose eigenvalues are greater than 1 (Kaiser's rule)

round(cumsum(eigval)/sum(eigval), 2) #cumulative proportion of variance
eigval #eigenvalues

#Interpret PCs: Look at loadings/eigenvectors to interpret components (bottom part of output)

summary(data31_pca, loadings=T)

#Conducting PCA manually
eig1 <- data31 %>% cor %>% eigen
eig1

#Plot of loadings (which vars contribute to which PCs). Better plots with factoextra
library(factoextra)
fviz_pca_biplot(data31_pca)

```

PC1 is the general strength axis: all loadings/coefs have similar sign and magnitude. The higher a state scores on PC1, all eight varaibles will be high. PC2 is an Income vs. Murder axis. Higher scores on PC2 mean high income and low Murder rate. Lower scores on PC2 mean the opposite: low income but high Murder rate. PC3 is an Education vs. UrbanPercent axis. Higher scores on PC3 mean high education and low UrbanPercent. Lower scores mean low education and high UrbanPercent. The total variance of the scaled data was 8.82, so total variance of PCs must be 8.82. The proportion of variance explained by PC1 is 0.4301744 and the proportion of variance explained by PC2 is 0.2840195. Therefore, the total variance of in the dataset explained by the PC1 is 3.794137864 and total variance explained by PC2 is 2.505052068.

###  Linear Classifier

```{R}

#The response variable for binary classification, high_low_UrbanPercent, is coded as a factor type: "high" or "low." I explicitly converted it to numeric so "high" is 1 and "low" is 0.
data32 <- data3 %>% mutate(high_low_UrbanPercent = ifelse(high_low_UrbanPercent=="high", 1, 0))
data32 

fit <- glm(high_low_UrbanPercent ~ income, data=data32, family = "binomial")
score <- predict(fit, type="response")
class_diag(score,data32$high_low_UrbanPercent,positive=1)

#Predict a binary variable (response) from ALL of the rest of the numeric variables in the dataset.
#Predict high_low_UrbanPercent (high=1, low=0) from each of the numeric variables

data32 %>% mutate(score=score) %>% ggplot(aes(income,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(young,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(urban,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(Murder,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(Assault, high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(Rape,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(UrbanPercent,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)

data32%>% mutate(score=score) %>% ggplot(aes(Total_Arrests,high_low_UrbanPercent))+geom_point(aes(color=score>.5))+
  geom_smooth(method="glm", se=F,method.args = list(family = "binomial"))+ylim(0,1)+geom_hline(yintercept=.5, lty=2)



print("--------------------------------------------------------------------------------------")
fit <- glm(high_low_UrbanPercent ~ income + young + urban + Assault + Murder + Rape + UrbanPercent + Total_Arrests, data=data32, family="binomial")
probs<-predict(fit, type="response")
score %>% round(3)
class_diag(score,truth=data32$high_low_UrbanPercent, positive=1)


print("----------------------------------------------------------")
table(truth = data32$high_low_UrbanPercent, prediction = probs>.5)

```

```{R}

Data3 <- data32 %>% select(-1)
Data3

y <- Data3$high_low_UrbanPercent
y <- factor(y, levels=c("high","low"))
x <- Data3$income


data33<-Data3%>%mutate(y=high_low_UrbanPercent)
data33$high_low_UrbanPercent<-NULL #remove this variable for now
data33

fit <- glm(Data3$high_low_UrbanPercent~income,data=Data3,family="binomial") #fit model
prob <- predict(fit,type="response") #get predicted probabilities
prob
class_diag(prob,data33$y,positive=1)

set.seed(1234)
#split the dataset in two random parts
one<-data33 %>% sample_frac(.5)
two<-data33 %>% anti_join(one)

## Train model on one part, test it on the other, get performance
fit<-glm(y~income,data=one,family="binomial")
probs1<-predict(fit,newdata = two,type="response")
diags1<-class_diag(probs1, two$y, positive=1)

## Switch the sets around and repeat the process
fit<-glm(y~income,data=two,family="binomial")
probs2<-predict(fit,newdata = one,type="response")
diags2<-class_diag(probs2, one$y, positive=1)
diags<-rbind(diags1,diags2)
diags

summarize_all(diags,mean) #average diagnostics across all k folds


set.seed(1234)
k=10 #choose number of folds

data34<-data33[sample(nrow(data33)),] #randomly order rows
folds<-cut(seq(1:nrow(data33)),breaks=k,labels=F) #create folds

diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data34[folds!=i,]
  test<-data34[folds==i,]
  truth<-test$y ## Truth labels for fold i
  
  ## Train model on training set (all but fold i)
  fit <- glm(y~income,data=train,family="binomial")
  ## Test model on test set (fold i)
  probs<-predict(fit,newdata = test,type="response")
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean) #average diagnostics across all k folds
```

The area under the AUC curve quantifies how well we are predicting overall. The AUC value after running the `class_diag` function is 0.852, which is a good value. I then performed k-fold CV on this same model. After running the `class_diag` function to get out-of-sample performance averaged across my k folds, my model predicts new observations per CV AUC very well since the AUC value came out to be 0.925. The goal of cross-validation is to test the model's ability to make predictions on new data that was not used to estimate it. Since my model improved in CV, there is no sign of overfitting.


### Non-Parametric Classifier

```{R}
library(caret)

Data3 
Data3 %>% select(1:9) %>% dist() %>% as.matrix %>% #euclidean distances among all datapoints
  as.data.frame %>% rownames_to_column %>% 
  pivot_longer(-1) %>% left_join(select(rownames_to_column(Data3),rowname,high_low_UrbanPercent), by="rowname") %>% 
  group_by(name) %>% slice_min(value, n=5) %>%#for each datapoint, grab the 5 closest points 
  summarize(y_hat=mean(high_low_UrbanPercent)) 

library(caret)
knn_fit <- knn3(factor(high_low_UrbanPercent==1,levels=c("TRUE","FALSE")) ~ education + income + young + urban + Assault + Murder + Rape + UrbanPercent + Total_Arrests, data=Data3, k=5)

y_hat_knn <- predict(knn_fit,Data3)

y_hat_knn

data.frame(y_hat_knn,names=rownames(Data3))%>% arrange(names) #The names are distinguished as numbers. Each number represents a State

table(truth= factor(Data3$high_low_UrbanPercent==1, levels=c("TRUE","FALSE")),
prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))

class_diag(y_hat_knn[,1],Data3$high_low_UrbanPercent, positive=1)

```

```{R}
# cross-validation of np classifier here

#K-Fold CV with kNN

set.seed(1234)
k=10 #choose number of folds

data<-data33[sample(nrow(data33)),] #randomly order rows
folds<-cut(seq(1:nrow(data33)),breaks=k,labels=F) #create 10 folds

diags<-NULL
for(i in 1:k){
  
  ## Create training and test sets
  train<-data33[folds!=i,]
  test<-data33[folds==i,]
  truth<-test$y
  
  ## Train model on training set
  fit<-knn3(y~.,data=train)
  probs<-predict(fit,newdata = test)[,2]
  
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}

summarize_all(diags,mean) #CV AUC a little worse with kNN

```

The CV AUC with kNN is 0.975, therefore the model is great at predicting new observations per CV AUC, and there are no signs of overfitting. It is also an improvement from CV AUC value with the linear model, which was 0.925.


### Regression/Numeric Prediction

```{R}
# regression model code here

fit<-lm(income~.,data=data33) #predict income from all other variables
yhat<-predict(fit) #predicted income

#Instead of classification diagnostics, calculate mean squared error (MSE)

mean((data33$income-yhat)^2) #mean squared error (MSE)


```

```{R}
# cross-validation of regression model here

set.seed(1234)
k=5 #choose number of folds
data<-data33[sample(nrow(data33)),] #randomly order rows
folds<-cut(seq(1:nrow(data33)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(income~.,data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error (MSE) for fold i
  diags<-mean((test$income-yhat)^2)
}
mean(diags) 



```

First, I predicted income from all the other variables, and instead of classification diagnostics, I calculated the mean squared error (MSE). The MSE is higher in CV, which is not good (means overfitting).

### Python 

```{R}

library(reticulate)
use_python("/usr/bin/python3")


print(py$data_py)


```

```{python}

print(r.data33.head())
data_py = r.data33


```

In the Python code chunk, I accessed an R-defined object, data33, with r. I used the function head() to print out the first few rows of the dataset. I created a new variable, data, and defined it as the R-defined dataset, data33. Then, I called data_py, the Python-defined object, in the R code chunk by using py$, which printed out the whole dataset. 

### Concluding Remarks

Include concluding remarks here, if any




